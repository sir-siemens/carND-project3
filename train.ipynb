{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### First read data from the disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import copy\n",
    "%matplotlib inline\n",
    "\n",
    "def load_training_data(directory):\n",
    "    df_raw = pd.read_csv('./'+directory+'/driving_log.csv')\n",
    "    df_raw.columns = ['center','left','right','steering','throttle','brake','speed']\n",
    "    directory_series = np.empty( len(df_raw), dtype=object)\n",
    "    directory_series[:] = directory\n",
    "    df_raw['directory'] = pd.Series( directory_series, index=df_raw.index)\n",
    "    return df_raw\n",
    "\n",
    "def plot_hist(dataframe, bins = 20):  \n",
    "    steering_cmds = dataframe.as_matrix(columns=['steering'])\n",
    "    steering_data = steering_cmds[:,0]\n",
    "    n, bins, patches = plt.hist(steering_data, bins, facecolor='green', alpha=0.75)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Read training data from different directories\n",
    "\n",
    "Here I use panda for training data IO and filter the data which the velocities are extremly low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "directory1 = 'training_data1'\n",
    "df_straight = load_training_data(directory1)\n",
    "directory2 = 'training_data_turn'\n",
    "df_turn = load_training_data(directory2)\n",
    "directory3 = 'training_data_turn2'\n",
    "df_turn2 = load_training_data(directory3)\n",
    "directory4 = 'training_data_3'\n",
    "df_train4 = load_training_data(directory4)\n",
    "\n",
    "# for fine tuning\n",
    "directory5 = 'traing_data_difficult_turn'\n",
    "df_train5 = load_training_data(directory5)\n",
    "directory6 = 'training_data_left_difficult_turn'\n",
    "df_train6 = load_training_data(directory6)\n",
    "\n",
    "# preprocessing throw all the data where speed is smaller than 2\n",
    "\n",
    "df_turn_preprocess = df_turn[ df_turn.speed > 5 ] \n",
    "df_straight_preprocess = df_straight[ ((df_straight.speed > 20.0) & (df_straight.speed < 30.0))] \n",
    "df_turn2_preprocess = df_turn2[ df_turn2.speed > 5 ] \n",
    "df_train4_preprocess = df_train4[ df_train4.speed > 3] \n",
    "df_train5_preprocess = df_train5[ df_train5.speed > 3] \n",
    "df_train6_preprocess = df_train6[ df_train6.speed > 3] \n",
    "#df[ ((df.steering > -0.3) & (df.steering < -0.25) )]\n",
    "\n",
    "\n",
    "frames = [df_turn_preprocess, df_straight_preprocess, df_turn2_preprocess, df_train4_preprocess]\n",
    "df = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "frames_fine_tune = [df_train5_preprocess,df_train6_preprocess ]\n",
    "df_fine_tune = pd.concat(frames_fine_tune, ignore_index=True)\n",
    "\n",
    "frames_fine_tune_right_turn = [df , df_fine_tune ]\n",
    "df_right_turn = pd.concat(frames_fine_tune_right_turn, ignore_index=True)\n",
    "# select right turn\n",
    "df_right_turn_preprocess = df_right_turn[ df_right_turn.steering > 0.15]\n",
    "\n",
    "plt.figure(figsize=(5,10))\n",
    "plt.subplot(6, 1, 1)\n",
    "plot_hist(df_straight_preprocess)\n",
    "plt.subplot(6, 1, 2)\n",
    "plot_hist(df_turn_preprocess)\n",
    "plt.subplot(6, 1, 3)\n",
    "plot_hist(df_turn2_preprocess)\n",
    "plt.subplot(6, 1, 4)\n",
    "plot_hist(df_train4_preprocess)\n",
    "plt.subplot(6, 1, 5)\n",
    "plot_hist(df)\n",
    "plt.subplot(6, 1, 6)\n",
    "plot_hist(df_train5_preprocess)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# spllit train validation \n",
    "# split df_merge into training and validation set\n",
    "msk = np.random.rand(len(df)) < 0.8\n",
    "train_samples = df[msk]\n",
    "validation_samples = df[~msk]\n",
    "print ('training n %d', len(train_samples))\n",
    "print ('validation n %d', len(validation_samples))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the correction steering angle for the left and right camera based on the current steering angle\n",
    "\n",
    "Here I realize this function with quadatic funtion. When steering angle is 0, I only apply 0.1 for the left or right camera image. When the car is turning, namely steering angle > 0, the training steering angle for left or right increases but does not exceed max_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# steering_angle: current steering angle, abs_max_turn: max steering angle that is  \n",
    "def compute_offset(steering_angle, abs_max_turn = 0.4):\n",
    "    max_offset = 0.4\n",
    "    min_offset = 0.1\n",
    "    param_a = (max_offset -  min_offset) / (abs_max_turn * abs_max_turn) # 0.4 is the approximate maximal turn \n",
    "    offset = param_a * steering_angle * steering_angle + min_offset\n",
    "    return offset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator function to pull data from the hard disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "def generator(samples, batch_size=32):\n",
    "    num_samples = len(samples)\n",
    "    while 1: # Loop forever so the generator never terminates\n",
    "        #shuffle(samples)\n",
    "        samples.apply(np.random.permutation)\n",
    "        #print (samples.speed[0:3])\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            df_batch_samples = samples[offset:offset+batch_size]\n",
    "            \n",
    "            images = []\n",
    "            angles = []\n",
    "            for row_index, series in df_batch_samples.iterrows():\n",
    "                name = './'+ series.directory+'/IMG/'+series.center.split('/')[-1]\n",
    "                center_image = cv2.imread(name)\n",
    "                center_angle = float(series.steering)\n",
    "                images.append(center_image)\n",
    "                angles.append(center_angle)\n",
    "                \n",
    "                # flip image \n",
    "                #center_image_flipped = np.fliplr(center_image)\n",
    "                #center_image_flipped_cmd = -center_angle\n",
    "                #images.append(center_image_flipped)\n",
    "                #angles.append(center_image_flipped_cmd)                \n",
    "            \n",
    "                left_name = './'+ series.directory+'/IMG/'+series.left.split('/')[-1]\n",
    "                left_image = cv2.imread(left_name)\n",
    "                left_angle = float(series.steering) + compute_offset(float(series.steering))\n",
    "                images.append(left_image)\n",
    "                angles.append(left_angle)\n",
    "\n",
    "                # flip left image \n",
    "                #left_image_flipped = np.fliplr(left_image)\n",
    "                #left_image_flipped_cmd = -left_angle\n",
    "                #images.append(left_image_flipped)\n",
    "                #angles.append(left_image_flipped_cmd)                \n",
    "\n",
    "            \n",
    "                right_name = './'+ series.directory+'/IMG/'+series.right.split('/')[-1]\n",
    "                right_image = cv2.imread(right_name)\n",
    "                right_angle = float(series.steering) - compute_offset(float(series.steering))\n",
    "                images.append(right_image)\n",
    "                angles.append(right_angle)\n",
    "                \n",
    "                #right_image_flipped = np.fliplr(right_image)\n",
    "                #right_image_flipped_cmd = -right_angle\n",
    "                #images.append(right_image_flipped)\n",
    "                #angles.append(right_image_flipped_cmd)\n",
    "           \n",
    "            \n",
    "            # trim image to only see section with road\n",
    "            X_train = np.array(images)\n",
    "            y_train = np.array(angles)\n",
    "           \n",
    "            yield sklearn.utils.shuffle(X_train, y_train)\n",
    "            #return sklearn.utils.shuffle(X_train, y_train)\n",
    "\n",
    "\n",
    "            \n",
    "# compile and train the model using the generator function\n",
    "\n",
    "train_generator = generator(train_samples, batch_size=128)\n",
    "validation_generator = generator(validation_samples, batch_size=128)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Visualize training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline\n",
    "\n",
    "(X_training, y_train) =  next(train_generator)\n",
    "\n",
    "random_image = random.randint(0 , X_training.shape[0])   \n",
    "#plt.figure( figsize =(100,100))\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.imshow( cv2.cvtColor(X_training[random_image], cv2.COLOR_BGR2RGB) )\n",
    "#plt.subplot(3, 1, 2)\n",
    "#plt.imshow( cv2.cvtColor(X_training_left[random_image], cv2.COLOR_BGR2RGB) )\n",
    "#plt.subplot(3, 1, 3)\n",
    "#plt.imshow( cv2.cvtColor(X_training_right[random_image], cv2.COLOR_BGR2RGB) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### The first model I tried\n",
    "This model is the model that I use for P2 traffic sign classification. However, it does not perform well for this task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Flatten, Dense , Lambda, MaxPooling2D, Dropout,Cropping2D\n",
    "\n",
    "i_shape = (160, 320,3)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Lambda(lambda x: x / 127.5 - 1.0, input_shape=i_shape ))\n",
    "model.add(Cropping2D(cropping=((70,25), (0,0)), input_shape=(160,320,3)))\n",
    "\n",
    "# conv_net\n",
    "model.add(Conv2D( 6, 5, 5, activation='relu' ))\n",
    "model.add(Conv2D( 6, 5, 5,  activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "########################################################################\n",
    "# conv_net\n",
    "model.add(Conv2D(16, 3, 3, activation='relu'))\n",
    "model.add(Conv2D(16, 3, 3, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#########################################################################\n",
    "model.add(Flatten())\n",
    "model.add(Dense(400, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(84, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mse',optimizer='adam')\n",
    "#model.fit(X_training, y_train, validation_split = 0.2, shuffle=True)\n",
    "model.fit_generator(train_generator, samples_per_epoch= 6*len(train_samples), validation_data=validation_generator, nb_val_samples=6*len(validation_samples), nb_epoch=5)\n",
    "model.save('first_model.h5')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Nvidia model that worked\n",
    "A small modification that I add to the model is the dropout layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Flatten, Dense , Lambda, MaxPooling2D, Dropout,Cropping2D\n",
    "\n",
    "i_shape = (160, 320,3)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Lambda(lambda x: x / 127.5 - 1.0, input_shape=i_shape ))\n",
    "model.add(Cropping2D(cropping=((70,25), (0,0)), input_shape=(160,320,3)))\n",
    "\n",
    "# conv_net\n",
    "model.add(Conv2D( 24, 5, 5, subsample=(2,2), activation='relu' ))\n",
    "model.add(Conv2D( 36, 5, 5, subsample=(2,2), activation='relu'))\n",
    "model.add(Conv2D( 48, 5, 5, subsample=(2,2), activation='relu'))\n",
    "# conv_net\n",
    "model.add(Conv2D(64, 3, 3, activation='relu'))\n",
    "model.add(Conv2D(64, 3, 3, activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100))\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(Dense(50))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mse',optimizer='adam')\n",
    "#model.fit(X_training, y_train, validation_split = 0.2, shuffle=True)\n",
    "model.fit_generator(train_generator, samples_per_epoch= 3 * len(train_samples), validation_data=validation_generator, nb_val_samples=3 * len(validation_samples), nb_epoch=5)\n",
    "model.save('nvidia_model.h5')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Here a couple of Fine tune steps are conducted \n",
    "The main problem is the car is not capable of making one left turn and one right turn (one at lake, the other right after the bridge). The number of these dataset is under presented in the whole dataset. I record new training set only on these two curves and train the network against these two curves. I also lower the learning rate and increase the training epoch and finally the car makes it through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('nvidia_model_fine_tune_3.h5')\n",
    "model.fit_generator(train_generator, samples_per_epoch= 3*len(train_samples), validation_data=validation_generator, nb_val_samples=3*len(validation_samples), nb_epoch=5)\n",
    "model.save('nvidia_model_fine_tune_4.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine tune on turn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Flatten, Dense , Lambda, MaxPooling2D, Dropout,Cropping2D\n",
    "from keras.optimizers import Adam\n",
    "# \n",
    "\n",
    "# spllit train validation \n",
    "# split df_merge into training and validation set\n",
    "msk = np.random.rand(len(df_right_turn_preprocess)) < 0.8\n",
    "train_samples = df_right_turn_preprocess[msk]\n",
    "validation_samples = df_right_turn_preprocess[~msk]\n",
    "print ('training n %d', len(train_samples))\n",
    "print ('validation n %d', len(validation_samples))\n",
    "\n",
    "\n",
    "train_generator = generator(train_samples, batch_size=32)\n",
    "validation_generator = generator(validation_samples, batch_size=32)\n",
    "\n",
    "\n",
    "model = load_model('nvidia_model_fine_tune_5.h5')\n",
    "\n",
    "model.compile(loss='mse',\n",
    "              optimizer= Adam(lr = 0.0002))\n",
    "\n",
    "model.fit_generator(train_generator, samples_per_epoch= 3*len(train_samples), validation_data=validation_generator, nb_val_samples=3*len(validation_samples), nb_epoch=50)\n",
    "model.save('nvidia_model_fine_tune_5.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Flatten, Dense , Lambda, MaxPooling2D, Dropout,Cropping2D\n",
    "from keras.optimizers import Adam\n",
    "# \n",
    "\n",
    "# spllit train validation \n",
    "# split df_merge into training and validation set\n",
    "msk = np.random.rand(len(df_train6_preprocess)) < 0.8\n",
    "train_samples = df_train6_preprocess[msk]\n",
    "validation_samples = df_train6_preprocess[~msk]\n",
    "print ('training n %d', len(train_samples))\n",
    "print ('validation n %d', len(validation_samples))\n",
    "\n",
    "\n",
    "train_generator = generator(train_samples, batch_size=32)\n",
    "validation_generator = generator(validation_samples, batch_size=32)\n",
    "\n",
    "\n",
    "model = load_model('nvidia_model_fine_tune_5.h5')\n",
    "model.compile(loss='mse',\n",
    "              optimizer= Adam(lr = 0.0001))\n",
    "model.fit_generator(train_generator, samples_per_epoch= 3*len(train_samples), validation_data=validation_generator, nb_val_samples=3*len(validation_samples), nb_epoch=50)\n",
    "model.save('nvidia_model_fine_tune_5.h5')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:carnd-term1]",
   "language": "python",
   "name": "conda-env-carnd-term1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
